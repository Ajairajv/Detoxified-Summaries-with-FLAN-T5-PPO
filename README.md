# Detoxified-Summaries-with-FLAN-T5-PPO

This notebook demonstrates how to fine-tune a **FLAN-T5** model to generate less toxic content. We'll utilize **Meta AI's hate speech reward model**, a binary classifier that predicts "not hate" or "hate," in conjunction with **Proximal Policy Optimization (PPO)** for reinforcement learning. This approach aims to reduce the model's toxicity in generated summaries.

---

## Table of Contents

1.  **Set Up Kernel and Required Dependencies**
2.  **Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator**
    * Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction
    * Prepare Reward Model
    * Evaluate Toxicity
3.  **Perform Fine-Tuning to Detoxify the Summaries**
    * Initialize `PPOTrainer`
    * Fine-Tune the Model
    * Evaluate the Model Quantitatively
    * Evaluate the Model Qualitatively

---

## 1. Set Up Kernel and Required Dependencies

This section covers the initial setup, including configuring the kernel and installing all necessary dependencies for the project.

---

## 2. Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator

Here, we'll load the pre-trained **FLAN-T5 model** that has already been fine-tuned for summarization. We'll then prepare **Meta AI's hate speech reward model** which will guide our detoxification process. Finally, we'll set up a **toxicity evaluator** to assess the level of toxicity in the generated summaries.

### Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction

We'll load the dataset and the FLAN-T5 model that has been previously fine-tuned for summarization tasks.

### Prepare Reward Model

This step involves setting up and loading Meta AI's pre-trained reward model, crucial for providing feedback during the PPO training.

### Evaluate Toxicity

Before fine-tuning, we'll evaluate the baseline toxicity of the summaries generated by the initial model.

---

## 3. Perform Fine-Tuning to Detoxify the Summaries

This is the core section where we apply reinforcement learning to reduce toxicity. We'll use **Proximal Policy Optimization (PPO)** to fine-tune the model, leveraging the feedback from the reward model.

### Initialize `PPOTrainer`

We'll set up the `PPOTrainer`, configuring it with the FLAN-T5 model and the reward model.

### Fine-Tune the Model

Here, we'll execute the PPO training loop to iteratively fine-tune the model, guiding it to generate less toxic summaries.

### Evaluate the Model Quantitatively

After fine-tuning, we'll quantitatively evaluate the model's performance, focusing on metrics related to toxicity reduction.

### Evaluate the Model Qualitatively

Finally, we'll perform a qualitative assessment of the generated summaries to understand the impact of detoxification on summary quality and coherence.
